{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 사이킷런(Scikit-Learn) / 학습 데이터 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사이킷런(scikit-learn)은 파이썬 머신러닝 라이브러리 중 가장 많이 사용되는 라이브러리로 다양한 머신러닝 모듈을 제공합니다.\n",
    "\n",
    "- **Example Data**\n",
    "    - sklearn.datasets : 예제 데이터 세트 제공\n",
    "---\n",
    "- **feature 처리**\n",
    "    - sklearn.preprocessing : 데이터 전처리에 필요한 다양한 가공 기능 제공\n",
    "    - sklearn.feature_selection : 중요한 feature를 우선순위로 선택하기 위한 수행 기능 제공\n",
    "    - sklearn.feature_extraction : 데이터의 벡터화된 feature 추출 기능 제공\n",
    "---\n",
    "- **feature 처리 & 차원 축소** \n",
    "    - sklearn.decomposition : 차원 축소와 관련된 알고리즘 제공\n",
    "---\n",
    "- **데이터 분리, 검증 & 파라미터 튜닝** \n",
    "    - sklearn.model_selection : 교차 검증을 위한 데이터 세트 분리(Train & Test), GridSearch)로 파라미터 추출 등 API 제공\n",
    "---\n",
    "- **평가 (Evaluation)** \n",
    "    - sklearn.metrics : Classification, Regression, Clustering 등 성능 측정 방법 제공\n",
    "---\n",
    "- **ML 알고리즘** \n",
    "    - sklearn.ensemble : 앙상블 알고리즘 제공\n",
    "    - sklearn.linear_model : 선형 회귀 및 로지스틱 회귀 등 Regression 관련 알고리즘 지원\n",
    "    - sklearn.naive_bayes : 나이브 베이즈 알고리즘 제공\n",
    "    - sklearn.neighbors : 최근접 이웃 알고리즘 제공\n",
    "    - sklearn.svm : Support Vector Machine 알고리즘 제공\n",
    "    - sklearn.tree : 의사 결정 트리 알고리즘 제공\n",
    "    - sklearn.cluster : 비지도 클러스터링 (Unsupervised Clustering) 알고리즘 제공\n",
    "---\n",
    "- **유틸리티** \n",
    "    - sklearn.pipeline : feature 처리 등의 변환과 ML 알고리즘 학습, 예측 등을 함께 묶어서 실행할 수 있는 유틸리티 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Split \n",
    "\n",
    "올바른 학습을 위해 데이터 세트를 Train data와 Test data로 나누어야 합니다.\n",
    "이를 위해 train_test_split() 함수를 사용합니다.\n",
    " \n",
    " --- \n",
    "- train_test_split(arrays, test_size, train_size, random_state, shuffle, stratify)\n",
    "\n",
    "(1) Parameter\n",
    "   - arrays : 분할시킬 데이터를 입력 (Python list, Numpy array, Pandas dataframe 등..)\n",
    "   - test_size : 테스트 데이터셋의 비율(float)이나 갯수(int) (default = 0.25)\n",
    "   - train_size : 학습 데이터셋의 비율(float)이나 갯수(int) (default = test_size의 나머지)\n",
    "   - random_state : 데이터 분할시 셔플이 이루어지는데 이를 위한 시드값 (int나 RandomState로 입력)\n",
    "   - shuffle : 셔플여부설정 (default = True)\n",
    "   - stratify : 지정한 Data의 비율을 유지한다. 예를 들어, Label Set인 Y가 25%의 0과 75%의 1로 이루어진 Binary Set일 때, stratify=Y로 설정하면 나누어진 데이터셋들도 0과 1을 각각 25%, 75%로 유지한 채 분할된다.\n",
    "\n",
    "(2) Return\n",
    "   - X_train, X_test, Y_train, Y_test : arrays에 데이터와 레이블을 둘 다 넣었을 경우의 반환이며, 데이터와 레이블의 순서쌍은 유지된다.\n",
    "   - X_train, X_test : arrays에 레이블 없이 데이터만 넣었을 경우의 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 정확도(accuracy) : 100.00%\n",
      "검증 정확도(accuracy) : 95.56%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Iris 데이터 세트 불러오기\n",
    "iris = load_iris()\n",
    "\n",
    "# Classification에 사용할 머신러닝 모델 불러오기\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "# 전체 데이터로 학습\n",
    "dt_clf.fit(iris.data, iris.target)\n",
    "\n",
    "# 전체 데이터로 검증\n",
    "pred = dt_clf.predict(iris.data)\n",
    "\n",
    "# 검증 정확도 출력\n",
    "# 학습한 데이터를 그대로 검증 데이터로 사용하면 정확도가 100%가 나오게 됩니다.\n",
    "# 그러나 올바른 학습과 검증의 과정이 아닙니다.\n",
    "print(\"검증 정확도(accuracy) : {0:.2f}%\".format(accuracy_score(iris.target, pred) * 100))\n",
    "\n",
    "\n",
    "# 전체 데이터를 학습 데이터와 검증 데이터로 나누어 머신러닝 모델을 평가합니다.\n",
    "# 데이터 세트를 나눔으로써 머신러닝 모델을 좀 더 일반화할 수 있습니다.\n",
    "X_train, X_test, Y_train, Y_test =  train_test_split(iris.data, iris.target, test_size = 0.3, random_state = 123, shuffle = True)# 데이터 분할\n",
    "\n",
    "# 학습 데이터로 모델 학습 진행\n",
    "dt_clf.fit(X_train, Y_train)  \n",
    "\n",
    "# 검증 데이터로 성능 평가\n",
    "pred = dt_clf.predict(X_test)\n",
    "\n",
    "# 검증 정확도 출력\n",
    "print(\"검증 정확도(accuracy) : {0:.2f}%\".format( accuracy_score(Y_test, pred) * 100))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### High Bias & High Variance\n",
    "\n",
    "   - High bias : 훈련 데이터를 제대로 표현하지 못함. **Underfit**\n",
    "   - High variance : 훈련 데이터를 지나치게 자세하게 표현. 새로운 데이터를 오히려 제대로 예측하지 못함. **Overfit**\n",
    "   - Validation : 데이터는 train 셋으로 학습한 모델이 Overfit (Sample Bias)되지 않았는지 확인\n",
    "\n",
    "### Validation\n",
    "  - 데이터 셋이 작다면 train/valid/test 세 개로 분류하기 어려움\n",
    "  - 데이터가 적을 때는 **Cross validation (교차검증)**사용\n",
    "  \n",
    "#### Hold Out\n",
    " - train / test 두 개로 나눔\n",
    "\n",
    "#### K-Fold\n",
    " - train 데이터셋을 K개로 나눈다.\n",
    " - K개 중 한 개를 valid, 나머지를 training 용으로 사용하여 학습\n",
    " - K개의 모델의 Hyper-parameter의 평균을 최종 결과로 사용한다.<br>\n",
    " +) 모든 데이터를 train/valid 용으로 사용 가능<br>\n",
    " +) 적은 데이터로도 높은 정확도<br>\n",
    " -) 느림<br>\n",
    " \n",
    " \n",
    " \n",
    " - KFold(n_splits)\n",
    "    - n_splits : fold 개수\n",
    "- split(X)\n",
    "    - X : Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter : 1 Cross-Validation Accuracy : 1.0, Train Data 크기 : 120, Test Data 크기 : 30\n",
      "Iter : 2 Cross-Validation Accuracy : 1.0, Train Data 크기 : 120, Test Data 크기 : 30\n",
      "Iter : 3 Cross-Validation Accuracy : 0.9, Train Data 크기 : 120, Test Data 크기 : 30\n",
      "Iter : 4 Cross-Validation Accuracy : 0.9333, Train Data 크기 : 120, Test Data 크기 : 30\n",
      "Iter : 5 Cross-Validation Accuracy : 0.7667, Train Data 크기 : 120, Test Data 크기 : 30\n",
      "평균 검증 정확도 :  0.9199999999999999\n"
     ]
    }
   ],
   "source": [
    "#[4-3] K-fold 교차 검증\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Iris 데이터 세트 불러오기\n",
    "iris = load_iris() \n",
    "# Classification에 사용할 모델 불러오기\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "# 몇 개의 Fold로 나눌지 결정합니다.\n",
    "n_iter = 0\n",
    "kfold = KFold(n_splits = 5)\n",
    "cv_accuracy = []\n",
    "\n",
    "for train_idx, test_idx in kfold.split(iris.data):    # Data를 K만큼 나누기\n",
    "    X_train, X_test = iris.data[train_idx], iris.data[test_idx] # 나눈 데이터 저장\n",
    "    y_train, y_test = iris.target[train_idx], iris.target[test_idx]\n",
    "\n",
    "    dt_clf.fit(X_train,y_train)     # 모델 학습\n",
    "    pred = dt_clf.predict(X_test)   # 검증 데이터로 결과 예측\n",
    "    n_iter += 1\n",
    "\n",
    "    accuracy = np.round(accuracy_score(y_test, pred), 4)    # 각 Iter 별 정확도 측정\n",
    "    train_size = X_train.shape[0]\n",
    "    test_size = X_test.shape[0]\n",
    "\n",
    "    print(\"Iter : {0} Cross-Validation Accuracy : {1}, Train Data 크기 : {2}, Test Data 크기 : {3}\"\n",
    "          .format(n_iter, accuracy, train_size, test_size))\n",
    "\n",
    "    cv_accuracy.append(accuracy)\n",
    "\n",
    "print(\"평균 검증 정확도 : \", np.mean(cv_accuracy)) # 평균 검증 정확도 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Stratified K-Fold\n",
    " - 각 fold가 특수하게 나뉜 경우에는 K-Fold 정확도가 좋지 않음\n",
    " - 그러므로 각 fold 데이터 클래스의 분포가 같도록 만들어준다.<br>\n",
    " +) 각 fold가 전체 데이터셋을 잘 대표<br>\n",
    " +) 모델을 학습시킬 때 편향되지 않게 학습 가능<br>\n",
    "\n",
    "\n",
    " - StratifiedKFold(n_splits)\n",
    "      - n_splits : fold 개수\n",
    "      \n",
    "      \n",
    " - split(X, y)\n",
    "      - X : Data\n",
    "      - y : label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 1\n",
      "--------------------\n",
      "학습 데이터 분포 : \n",
      " 2    33\n",
      "1    33\n",
      "0    33\n",
      "Name: label, dtype: int64\n",
      "--------------------\n",
      "검증 데이터 분포 : \n",
      " 2    17\n",
      "1    17\n",
      "0    17\n",
      "Name: label, dtype: int64\n",
      "--------------------\n",
      "Iter : 1, 정확도 : 98.04%, 학습 데이터 개수 : 99, 검증 데이터 개수 : 51\n",
      "\n",
      "Iteration : 2\n",
      "--------------------\n",
      "학습 데이터 분포 : \n",
      " 2    33\n",
      "1    33\n",
      "0    33\n",
      "Name: label, dtype: int64\n",
      "--------------------\n",
      "검증 데이터 분포 : \n",
      " 2    17\n",
      "1    17\n",
      "0    17\n",
      "Name: label, dtype: int64\n",
      "--------------------\n",
      "Iter : 2, 정확도 : 92.16%, 학습 데이터 개수 : 99, 검증 데이터 개수 : 51\n",
      "\n",
      "Iteration : 3\n",
      "--------------------\n",
      "학습 데이터 분포 : \n",
      " 2    34\n",
      "1    34\n",
      "0    34\n",
      "Name: label, dtype: int64\n",
      "--------------------\n",
      "검증 데이터 분포 : \n",
      " 2    16\n",
      "1    16\n",
      "0    16\n",
      "Name: label, dtype: int64\n",
      "--------------------\n",
      "Iter : 3, 정확도 : 100.0%, 학습 데이터 개수 : 102, 검증 데이터 개수 : 48\n",
      "\n",
      "평균 검증 정확도 : 96.7333%\n"
     ]
    }
   ],
   "source": [
    "#[4-4] Stratified K-Fold 교차 검증\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Iris 데이터 세트 불러오기\n",
    "iris = load_iris()\n",
    "\n",
    "# 데이터 분포를 확인하기 위해 DataFrame을 만들어줍니다.\n",
    "iris_df = pd.DataFrame(data = iris.data, columns = iris.feature_names)\n",
    "iris_df['label'] = iris.target\n",
    "\n",
    "# Classification에 사용할 모델 불러오기\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "# Iris 데이터는 3개의 Class로 이루어져 있습니다.\n",
    "# 2. 그러므로 3개의 Fold로 데이터를 나눕니다.\n",
    "n_iter = 0\n",
    "skf = StratifiedKFold(n_splits = 3)\n",
    "avg_acc = []\n",
    "\n",
    "for train_idx, test_idx in skf.split(iris.data,iris.target):    # iris 데이터에서 나누기\n",
    "    # Iter 수 증가\n",
    "    n_iter += 1\n",
    "    \n",
    "    # 3. K 개수 만큼 Fold 나누기\n",
    "    train_label = iris_df['label'].iloc[train_idx]                  \n",
    "    test_label = iris_df['label'].iloc[test_idx]\n",
    "    X_train, X_test = iris.data[train_idx],iris.data[test_idx]\n",
    "    y_train, y_test = iris.target[train_idx],iris.target[test_idx]\n",
    "    \n",
    "    print(\"Iteration :\", n_iter)\n",
    "    print(\"--------------------\")\n",
    "    print(\"학습 데이터 분포 : \\n\", train_label.value_counts())\n",
    "    print(\"--------------------\")\n",
    "    print(\"검증 데이터 분포 : \\n\", test_label.value_counts())\n",
    "    print(\"--------------------\")\n",
    "    \n",
    "    # 4. 학습 데이터로모델 학습\n",
    "    dt_clf.fit(X_train,y_train)\n",
    "    \n",
    "    # 검증 데이터로 성능 평가\n",
    "    pred = dt_clf.predict(X_test)\n",
    "\n",
    "    accuracy = np.round(accuracy_score(y_test, pred), 4)\n",
    "    train_size = X_train.shape[0]\n",
    "    test_size = X_test.shape[0]\n",
    "\n",
    "    print(\"Iter : {0}, 정확도 : {1}%, 학습 데이터 개수 : {2}, 검증 데이터 개수 : {3}\\n\"\n",
    "          .format(n_iter, accuracy*100, train_size, test_size))\n",
    "\n",
    "    avg_acc.append(accuracy)\n",
    "    \n",
    "print(\"평균 검증 정확도 : {0:.4f}%\".format(np.mean(avg_acc)* 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Leave One Out (LOO)\n",
    "\n",
    "- K-Fold의 특수한 경우\n",
    "- K (데이터의 총 개수), 각 fold에는 데이터 한 개만<br>\n",
    "+) 작은 데이터셋에서 좋은 결과<br>\n",
    "-) 데이터셋이 커질수록 매우 느려짐<br>\n",
    "\n",
    "#### Leave P Out\n",
    "\n",
    "- LOO와 비슷하지만, 여기서는 p개 만큼 제외하고 train/test셋을 만든다.\n",
    "- 총 N개의 데이터에 대해 train_size=N-p, valid_size=p \n",
    "- 총 N개의 데이터에 대해 nCp 만큼 연산. LOO보다 더 느리다.\n",
    "\n",
    "#### Shuffle - Split\n",
    "\n",
    "- 반복 횟수를 train fold 수, valid fold 개수와 독립적으로 조절\n",
    "- train / valid fold의 합을 전체 fold 수와 다르게 설정도 가능\n",
    "\n",
    "#### Bootstrap\n",
    "\n",
    "- 데이터가 적어서 모집단을 추정하기 어려울 때, 데이터들의 분산, 평균, 편차 등을 구하는 통계기법\n",
    "- 데이터 양이 적을 때, 모델의 통계적 신뢰도를 높이기 위함\n",
    "- 데이터 셋 내에서 복원추출 방법을 이용해 새로운 샘플을 만든다\n",
    "- 충분히 많은 샘플이 생긴다면 모집단의 통계적 추정이 가능"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
