{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6-1. Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Function\n",
    "\n",
    "ref) https://sshkim.tistory.com/146\n",
    "\n",
    "- softmax 함수는 K개의 값이 존재할 때 각각의 값의 편차를 확대시켜 큰 값은 상대적으로 더 크게, 작은 값은 상대적으로 더 작게 만든 다음에 normalization 시키는 함수이다.\n",
    "\n",
    "- 모든 입력신호로부터 영향을 받는다는 특징이 있다. 즉, '확률적 해석'이 가능하다.\n",
    "<img src=\"./img/s.png\" width=\"500\" height=\"500\"></img>\n",
    "<img src=\"./img/s2.png\" width=\"500\" height=\"500\"></img>\n",
    "<img src=\"./img/s3.png\" width=\"500\" height=\"500\"></img><br>\n",
    "\n",
    "- ONE-Hot Encoding을 사용하여, 가장 큰 값은 1로 두고, 나머지는 0으로 변환할 수 있다. 텐서플로우에서는 argmax라는 함수를 사용하여 ONE-Hot Encoding을 적용할 수 있다.\n",
    "\n",
    "<img src=\"./img/s4.png\" width=\"500\" height=\"500\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logit, sigmoid, softmax의 관계\n",
    "\n",
    "ref) https://opentutorials.org/module/3653/22995\n",
    "\n",
    "<img src=\"./img/관계도.png\" width=\"700\" height=\"700\"></img>\n",
    "\n",
    "- logit(logistic+probit)과 sigmoid는 서로 역함수 관계\n",
    "- 2개 클래스를 대상으로 정의하던 logit을 K개의 클래스를 대상으로 일반화하면 softmax함수가 유도됨.\n",
    "- softmax함수에서 K를 2로 놓으면 sigmoid함수로 환원이 되고, \n",
    "- 반대로 sigmoid함수를 K개의 클래스를 대상으로 일반화하면 softmax함수가 유도됨\n",
    "\n",
    "sigmoid함수는 인공신경망에서 ReLU가 등장하기 이전에 활발하게 사용되었던 activation function(활성화 함수)이고, hidden 노드 바로 뒤에 부착된다.\n",
    "softmax함수는 인공신경망이 내놓은 K개의 클래스 구분 결과를 확률처럼 해석하도록 만들어준다. 보통은 output 노드 바로 뒤에 부착됨.\n",
    "코드에서 사용될때는 서로 다른 용도(sigmoid는 activation에, softmax는 classification에)로 사용되지만\n",
    "수학적으로는 서로 같은 함수이다. 다루는 클래스가 2개냐 K개냐로 차이가 있을 뿐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM(Support Vector Machine)\n",
    "\n",
    "ref) https://ratsgo.github.io/machine%20learning/2017/05/23/SVM/\n",
    "\n",
    "두 범주를 나누는 분류 문제를 푼다고 가정할 때, 분류 경계면 사이의 거리를 **마진(margin)** 이라고 한다.\n",
    "<img src=\"./img/margin.png\" width=\"200\" height=\"200\"></img>\n",
    "SVM은 이 마진을 최대화하는 분류 경계면을 찾는 기법이다.\n",
    "<img src=\"./img/margin2.png\" width=\"200\" height=\"200\"></img>\n",
    "\n",
    "### Decision Boundary Margin\n",
    "\n",
    "<img src=\"./img/sv.png\" width=\"200\" height=\"200\"></img>\n",
    "(support vector들로 정해진 decision boundary가 가장 최적의 boundary이며, 현재 가지고 있는 데이터만으로 새로운 샘플이 들어왔을 때 일반화를 가장 잘 할 수 있는 decision rule을 찾을 수 있게 된 것임)\n",
    "\n",
    "- class를 분류하는 기준선에 여유를 둘 수 있다.\n",
    "- Large Margin Classifier\n",
    "- **Support Vector** : 결정 경계에 가장 가까운 데이터\n",
    "- slack 변수를 이용해 어느 정도 오차를 허용하기도 함 (Soft Margin 방법)\n",
    "\n",
    "### Kernel SVM\n",
    "\n",
    "ref) https://ratsgo.github.io/machine%20learning/2017/05/30/SVM3/\n",
    "https://bskyvision.com/163\n",
    "\n",
    "<img src=\"./img/kernel.png\" width=\"500\" height=\"500\"></img>\n",
    "\n",
    "- Non-linear Decision Boundary. 즉, 데이터를 구분 짓는 '선'을 긋기 힘들 때, 데이터를 선형으로 구분할 수 있는 공간으로 재배치한다. 이때 데이터를 재배치 해주는 함수가 **Kernel**\n",
    "\n",
    "- Polynomial, Sigmoid, Gaussian RBF\n",
    "\n",
    "<img src=\"./img/kernel2.png\" width=\"500\" height=\"500\"></img>\n",
    "\n",
    "<img src=\"./img/kernel3.png\" width=\"500\" height=\"500\"></img>\n",
    "\n",
    "**[매개변수 C의 영향]**\n",
    "\n",
    "<img src=\"./img/kernel4.png\" width=\"400\" height=\"400\"></img>\n",
    "\n",
    "<img src=\"./img/kernel5.png\" width=\"400\" height=\"400\"></img>\n",
    "\n",
    "- 'C' 값이 낮으면 일반화를 더 잘함. 높게 설정하면 반대로 이상치의 존재 가능성을 작게 봐서 좀 더 세심하게 결정 경계를 찾아낸다.\n",
    "- 'gamma'는 하나의 데이터 샘플이 영향력을 행사하는 거리를 결정한다. gamma는 가우시안 함수의 표준편차와 관련되어 있는데, 클수록 작은 표준편차를 갖는다. 즉, gamma가 클수록 한 데이터 포인터들이 영향력을 행사하는 거리가 짧아지는 반면, gamma가 낮을수록 커진다.\n",
    "- C는 데이터 샘플들이 다른 클래스에 놓이는 것을 허용하는 정도를 결정하고, gamma는 결정 경계의 곡률을 결정한다.\n",
    "- 참고로 C의 유무에 따라 하드마진(hard-margin) SVM, 소프트마진(soft-margin) SVM라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 나이브 베이즈 분류 (Naive Bayes Classifier)\n",
    "\n",
    "나이브 베이즈 분류는 데이터의 확률적 속성을 가지고 클래스를 판단하는 꽤 높은 성능을 가지는 머신러닝 알고리즘이다. \n",
    "\n",
    "<img src=\"./img/bayes.png\" width=\"400\" height=\"400\"></img>\n",
    "\n",
    "**장점**\n",
    "- Multi-class 분류에서 쉽고 빠르게 예측 가능\n",
    "- 각 특징들이 독립이라면 다른 분류 방식에 비해 결과가 좋고, 학습 데이터도 적게 필요\n",
    "\n",
    "**단점**\n",
    "- 각 특징들이 독립이 아니라면 결과의 신뢰성 하락\n",
    "- 학습 데이터에 없는 범주의 데이터가 들어오면 정상적인 예측 불가능"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
