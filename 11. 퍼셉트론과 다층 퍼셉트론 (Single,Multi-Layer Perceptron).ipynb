{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11장. 퍼셉트론과 다층 퍼셉트론(Single/Multi-Layer Perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [참고] 활성화 함수\n",
    "\n",
    "어떠한 신호를 입력받아 이를 적절한 처리를 하여 출력해주는 함수.\n",
    "<img src=\"./img/11-2.png\" width=\"300\" height=\"300\"></img>\n",
    "\n",
    "---\n",
    "- **step function**\n",
    "\n",
    "계단 모양, 임계값을 기준으로 활성화 되거나 혹은 비활성화 되는 형태.\n",
    "<img src=\"./img/11-8.png\" width=\"100\" height=\"100\"></img>\n",
    "                           \n",
    "<img src=\"./img/11-4.png\" width=\"400\" height=\"400\"></img>\n",
    "\n",
    "---\n",
    "- **sigmoid function**\n",
    "\n",
    "항상 0과 1사이의 값만 가질 수 있도록 하는 비선형 함수\n",
    "<img src=\"./img/11-14.png\"></img>\n",
    "<img src=\"./img/11-10.png\"></img>\n",
    "시그모이드 함수는 완전히 값을 전달하지 않거나(0) 혹은 완전히 전달한다(1)는 특성 때문에 실제 인체의 뉴런과 유사하다고 생각되어 널리 사용되었으나, 현재는 아래와 같은 이유로 점차 사용하지 않는 추세.\n",
    "\n",
    "   1. Vanishing Gradient <br>\n",
    "   \n",
    ": backpropagation에서 결과를 전달할 때 sigmoid를 사용한다. 그런데, sigmoid는 전달된 값을 0과 1 사이로 심하게 변형을 한다. 일정 비율로 줄어들기 때문에 왜곡은 아니지만, 값이 현저하게 작아지는 현상이 벌어진다. 3개의 layer를 거치면서 계속해서 1/10로 줄어들었다면, 현재 사용 중인 값은 처음 값의 1/1000이 된다. 이렇게 작아진 값을 갖고 원래의 영향력을 그대로 복원한다는 것은 불가능하다.<br>\n",
    "\n",
    "   2. 중심값이 0이 아니다.\n",
    "    \n",
    "---\n",
    "- **tanh function**\n",
    "\n",
    "tanh(hyperbolic tangent) function은 sigmoid 처럼 비선형 함수이지만 결과값의 범위가 -1부터 1이기 때문에 sigmoid와 달리 중심값이 0임. 따라서 sigmoid보다 optimazation이 빠르다는 장점이 있고 선호되지만 여전히 Vanishing Gradient 문제가 발생하기 때문에 대안이 등장함.\n",
    "\n",
    "<img src=\"./img/11-13.png\"></img>\n",
    "<img src=\"./img/11-12.png\"></img>\n",
    "\n",
    "---\n",
    "- **ReLU function**\n",
    "\n",
    "Rectified Linear Unit의 약자. 기존의 linear 함수인 sigmoid를 개선했다는 뜻. Vanishing Gradient문제를 해결할 수 있으며,미분이 아주 간단하게 됨\n",
    "\n",
    "<img src=\"./img/11-16.png\"></img>\n",
    "<img src=\"./img/11-11.png\"></img>\n",
    "\n",
    "0보다 작을 때는 0을 사용하고, 0보다 큰 값에 대해서는 해당 값을 그대로 사용하는 방법.\n",
    "**장점**\n",
    "- 기존의 sigmoid, tanh에 비해 converge(수렴)되는 속도가 6배 가까이 빠름. 이것은 그래프의 형태가 선형이고, saturate problem이 발생하지 않기 때문으로 보여짐. \n",
    "\n",
    "*saturate problem : weight의 업데이트가 멈추는 현상\n",
    "\n",
    "- x값이 0을 기준으로 선형발현/미발현 이라는 간단한 형태이기 때문에 상대적으로 연산량이 많은 exponential을 사용하지 않아, 컴퓨터의 연산에 대한 부담을 줄여줌\n",
    "\n",
    "**단점**\n",
    "\n",
    "- 하지만 입력값이 <0 일시, 함수 미분값이 0이 되는 약점이 있음.\n",
    "- ReLU의 output(결과값)이 0 or (+) / ReLU의 편미분 값도 0 or 1(+). 즉, sigmoid처럼 둘 다 양수라서 zigzag 현상이 생긴다. \n",
    "\n",
    "---\n",
    "- **PReLU (Parametric Rectifier or Leaky ReLU)**\n",
    "\n",
    "ReLU의 약점을 보안한 함수. 하지만 상대적으로 ReLU가 더 인기\n",
    "(Relu의 입력값이 < 0 일때의 약점 보안함)\n",
    "\n",
    "---\n",
    "- **ELU (Exponential Linear Units)**\n",
    "\n",
    "    - ReLU의 모든 장점 포함.\n",
    "    - exp() 함수를 계산해야 하는 약점.\n",
    "\n",
    "---\n",
    "- **Maxout**\n",
    "\n",
    "    - ReLU와 Leaky ReLU 를 일반화한 함수. 인기는 별로\n",
    "    - max(w1x + b1, w2x+b2)\n",
    "    - 위에 식처럼,activation 함수가 w1,w2,b1,b2 파라미터를 더 추가함. 결국 전체 네트워크의 파라미터 개수가 2배 증가 -> 사용하지 말것.\n",
    "\n",
    "ref)https://nittaku.tistory.com/267 \n",
    "https://pozalabs.github.io/Activation_Function/\n",
    "https://pythonkim.tistory.com/40\n",
    "https://nittaku.tistory.com/267\n",
    "https://jleewebblog.wordpress.com/2016/10/26/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%97%90%EC%84%9C-%ED%99%9C%EC%84%B1-%ED%95%A8%EC%88%98-%EA%B0%80%EC%9D%B4%EB%93%9C-activation-function-guide-in-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## [Softmax 활성화 함수 구현해보기]\n",
    "\n",
    "<img src=\"./img/11-24.png\" width=\"300\" height=\"300\"></img>\n",
    "\n",
    "- 여러개의 클래스를 구분해주는 알고리즘에서 쓰이는 활성화 함수.\n",
    "- sigmoid 함수의 일반화된 형태\n",
    "- 각 클래스에 대해서 정규화된(normalized) 확률값을 출력해냄."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax 분류 결과 : [0.08  0.    0.247 0.673 0.   ]\n",
      "클래스 예측 결과 가장 높은 값은 0.673로, 4번째 클래스입니다.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# numpy를 활용해 softmax 함수 구현\n",
    "def softmax(z):\n",
    "    return np.exp(z)/np.sum(np.exp(z))\n",
    "\n",
    "# 5개의 Class에 대한 점수\n",
    "class_s = np.array([12.77845895,5.28312333,13.9086965,14.91212438,0.70120341])\n",
    "\n",
    "# Softmax Activation Function을 거친 결과를 저장\n",
    "result = softmax(class_s)\n",
    "\n",
    "# Softmax를 거친 결과 중 가장 높은 값을 추출\n",
    "max_idx = np.argmax(result)\n",
    "max_value = np.max(result)\n",
    "\n",
    "print('Softmax 분류 결과 :', np.round(result,3))\n",
    "print('클래스 예측 결과 가장 높은 값은 {}로, {}번째 클래스입니다.'.format(np.round(max_value,3),max_idx+1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [참고] numpy 최소, 최대, 조건 색인값 : np.argmin(), np.argmax(), np.where()\n",
    "\n",
    "ref) https://rfriend.tistory.com/356 [R, Python 분석과 프로그래밍 (by R Friend)]\n",
    "\n",
    "- 최소값, 최대값, 혹은 조건에 해당하는 색인(index) 값을 찾기 \n",
    "\n",
    "   : np.argmin(), np.argmax(), np.where()\n",
    "\n",
    "\n",
    "\n",
    "- 최소값, 최대값, 혹은 조건에 맞는 값 찾기 \n",
    "\n",
    "   : np.min(), np.max(), x[np.where()]\n",
    "\n",
    "<img src=\"./img/11-17.jfif\"></img>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
